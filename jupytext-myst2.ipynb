{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains round-trip conversion between\n",
    "myst formatted text documents and notebooks.\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from textwrap import dedent\n",
    "\n",
    "import nbformat as nbf\n",
    "import yaml\n",
    "\n",
    "from jupytext.cell_to_text import three_backticks_or_more\n",
    "\n",
    "try:\n",
    "    from markdown_it import MarkdownIt\n",
    "    from mdit_py_plugins.front_matter import front_matter_plugin\n",
    "    from mdit_py_plugins.myst_blocks import myst_block_plugin\n",
    "    from mdit_py_plugins.myst_role import myst_role_plugin\n",
    "except ImportError:\n",
    "    MarkdownIt = None\n",
    "\n",
    "MYST_FORMAT_NAME = \"myst\"\n",
    "CODE_DIRECTIVE = \"{code-cell}\"\n",
    "RAW_DIRECTIVE = \"{raw-cell}\"\n",
    "\n",
    "\n",
    "def is_myst_available():\n",
    "    \"\"\"Whether the markdown-it-py package is available.\"\"\"\n",
    "    return MarkdownIt is not None\n",
    "\n",
    "\n",
    "def raise_if_myst_is_not_available():\n",
    "    if not is_myst_available():\n",
    "        raise ImportError(\n",
    "            \"The MyST Markdown format requires python >= 3.6 and markdown-it-py~=1.0\"\n",
    "        )\n",
    "\n",
    "\n",
    "def myst_version():\n",
    "    \"\"\"The version of myst.\"\"\"\n",
    "    return 0.13\n",
    "\n",
    "\n",
    "def myst_extensions(no_md=False):\n",
    "    \"\"\"The allowed extensions for the myst format.\"\"\"\n",
    "    if no_md:\n",
    "        return [\".myst\", \".mystnb\", \".mnb\"]\n",
    "    return [\".md\", \".myst\", \".mystnb\", \".mnb\"]\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Return the markdown-it parser to use.\"\"\"\n",
    "    parser = (\n",
    "        MarkdownIt(\"commonmark\")\n",
    "        .enable(\"table\")\n",
    "        .use(front_matter_plugin)\n",
    "        .use(myst_block_plugin)\n",
    "        .use(myst_role_plugin)\n",
    "        # we only need to parse block level components (for efficiency)\n",
    "        .disable(\"inline\", True)\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def matches_mystnb(\n",
    "    text,\n",
    "    ext=None,\n",
    "    requires_meta=True,\n",
    "    code_directive=CODE_DIRECTIVE,\n",
    "    raw_directive=RAW_DIRECTIVE,\n",
    "):\n",
    "    \"\"\"Attempt to distinguish a file as myst, only given its extension and content.\n",
    "\n",
    "    :param ext: the extension of the file\n",
    "    :param requires_meta: requires the file to contain top matter metadata\n",
    "    :param code_directive: the name of the directive to search for containing code cells\n",
    "    :param raw_directive: the name of the directive to search for containing raw cells\n",
    "    \"\"\"\n",
    "    # is the extension uniquely associated with myst (i.e. not just .md)\n",
    "    if ext and \".\" + (\".\" + ext).rsplit(\".\", 1)[1] in myst_extensions(no_md=True):\n",
    "        return True\n",
    "\n",
    "    # might the text contain metadata front matter\n",
    "    if requires_meta and not text.startswith(\"---\"):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        tokens = get_parser().parse(text + \"\\n\")\n",
    "    except (TypeError, ValueError) as err:\n",
    "        warnings.warn(f\"myst-parser failed unexpectedly: {err}\")  # pragma: no cover\n",
    "        return False\n",
    "\n",
    "    # Is the format information available in the jupytext text representation?\n",
    "    if tokens and tokens[0].type == \"front_matter\":\n",
    "        try:\n",
    "            metadata = yaml.safe_load(tokens[0].content)\n",
    "        except (yaml.parser.ParserError, yaml.scanner.ScannerError):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                if (\n",
    "                    metadata.get(\"jupytext\", {})\n",
    "                    .get(\"text_representation\", {})\n",
    "                    .get(\"format_name\", \"\")\n",
    "                    == MYST_FORMAT_NAME\n",
    "                ):\n",
    "                    return True\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    # is there at least on fenced code block with a code/raw directive language\n",
    "    for token in tokens:\n",
    "        if token.type == \"fence\" and (\n",
    "            token.info.startswith(code_directive)\n",
    "            or token.info.startswith(raw_directive)\n",
    "        ):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "class CompactDumper(yaml.SafeDumper):\n",
    "    \"\"\"This YAML dumper creates a more compact style for lists\"\"\"\n",
    "\n",
    "\n",
    "def represent_list(self, data):\n",
    "    \"\"\"Compact lists\"\"\"\n",
    "    flow_style = not any(isinstance(i, dict) for i in data)\n",
    "    return self.represent_sequence(\"tag:yaml.org,2002:seq\", data, flow_style=flow_style)\n",
    "\n",
    "\n",
    "def represent_dict(self, data):\n",
    "    \"\"\"Compact dicts\"\"\"\n",
    "    return self.represent_mapping(\"tag:yaml.org,2002:map\", data, flow_style=False)\n",
    "\n",
    "\n",
    "CompactDumper.add_representer(list, represent_list)\n",
    "CompactDumper.add_representer(dict, represent_dict)\n",
    "\n",
    "\n",
    "def dump_yaml_blocks(data, compact=True):\n",
    "    \"\"\"Where possible, we try to use a more compact metadata style.\n",
    "\n",
    "    For blocks with no nested dicts, the block is denoted by starting colons::\n",
    "\n",
    "        :other: true\n",
    "        :tags: [hide-output, show-input]\n",
    "\n",
    "    For blocks with nesting the block is enlosed by ``---``::\n",
    "\n",
    "        ---\n",
    "        other:\n",
    "            more: true\n",
    "        tags: [hide-output, show-input]\n",
    "        ---\n",
    "    \"\"\"\n",
    "    string = yaml.dump(data, Dumper=CompactDumper)\n",
    "    lines = string.splitlines()\n",
    "    if compact and all(line and line[0].isalpha() for line in lines):\n",
    "        return \"\\n\".join([f\":{line}\" for line in lines]) + \"\\n\\n\"\n",
    "    return f\"---\\n{string}---\\n\"\n",
    "\n",
    "\n",
    "def from_nbnode(value):\n",
    "    \"\"\"Recursively convert NotebookNode to dict.\"\"\"\n",
    "    if isinstance(value, nbf.NotebookNode):\n",
    "        return {k: from_nbnode(v) for k, v in value.items()}\n",
    "    return value\n",
    "\n",
    "\n",
    "class MystMetadataParsingError(Exception):\n",
    "    \"\"\"Error when parsing metadata from myst formatted text\"\"\"\n",
    "\n",
    "\n",
    "def strip_blank_lines(text):\n",
    "    \"\"\"Remove initial blank lines\"\"\"\n",
    "    text = text.rstrip()\n",
    "    while text and text.startswith(\"\\n\"):\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_fenced_cell(token, cell_index, cell_type):\n",
    "    \"\"\"Parse (and validate) the full directive text.\"\"\"\n",
    "    content = token.content\n",
    "    error_msg = \"{} cell {} at line {} could not be read: \".format(\n",
    "        cell_type, cell_index, token.map[0] + 1\n",
    "    )\n",
    "\n",
    "    body_lines, options = parse_directive_options(content, error_msg)\n",
    "\n",
    "    # remove first line of body if blank\n",
    "    # this is to allow space between the options and the content\n",
    "    if body_lines and not body_lines[0].strip():\n",
    "        body_lines = body_lines[1:]\n",
    "\n",
    "    return options, body_lines\n",
    "\n",
    "\n",
    "def parse_directive_options(content, error_msg):\n",
    "    \"\"\"Parse (and validate) the directive option section.\"\"\"\n",
    "    options = {}\n",
    "    if content.startswith(\"---\"):\n",
    "        content = \"\\n\".join(content.splitlines()[1:])\n",
    "        match = re.search(r\"^-{3,}\", content, re.MULTILINE)\n",
    "        if match:\n",
    "            yaml_block = content[: match.start()]\n",
    "            content = content[match.end() + 1 :]\n",
    "        else:\n",
    "            yaml_block = content\n",
    "            content = \"\"\n",
    "        yaml_block = dedent(yaml_block)\n",
    "        try:\n",
    "            options = yaml.safe_load(yaml_block) or {}\n",
    "        except (yaml.parser.ParserError, yaml.scanner.ScannerError) as error:\n",
    "            raise MystMetadataParsingError(error_msg + \"Invalid YAML; \" + str(error))\n",
    "    elif content.lstrip().startswith(\":\"):\n",
    "        content_lines = content.splitlines()  # type: list\n",
    "        yaml_lines = []\n",
    "        while content_lines:\n",
    "            if not content_lines[0].lstrip().startswith(\":\"):\n",
    "                break\n",
    "            yaml_lines.append(content_lines.pop(0).lstrip()[1:])\n",
    "        yaml_block = \"\\n\".join(yaml_lines)\n",
    "        content = \"\\n\".join(content_lines)\n",
    "        try:\n",
    "            options = yaml.safe_load(yaml_block) or {}\n",
    "        except (yaml.parser.ParserError, yaml.scanner.ScannerError) as error:\n",
    "            raise MystMetadataParsingError(error_msg + \"Invalid YAML; \" + str(error))\n",
    "\n",
    "    return content.splitlines(), options\n",
    "\n",
    "\n",
    "def read_cell_metadata(token, cell_index):\n",
    "    \"\"\"Return cell metadata\"\"\"\n",
    "    metadata = {}\n",
    "    if token.content:\n",
    "        try:\n",
    "            metadata = json.loads(token.content.strip())\n",
    "        except Exception as err:\n",
    "            raise MystMetadataParsingError(\n",
    "                \"Markdown cell {} at line {} could not be read: {}\".format(\n",
    "                    cell_index, token.map[0] + 1, err\n",
    "                )\n",
    "            )\n",
    "        if not isinstance(metadata, dict):\n",
    "            raise MystMetadataParsingError(\n",
    "                \"Markdown cell {} at line {} is not a dict\".format(\n",
    "                    cell_index, token.map[0] + 1\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def myst_to_notebook(\n",
    "    text,\n",
    "    code_directive=CODE_DIRECTIVE,\n",
    "    raw_directive=RAW_DIRECTIVE,\n",
    "    add_source_map=False,\n",
    "):\n",
    "    \"\"\"Convert text written in the myst format to a notebook.\n",
    "\n",
    "    :param text: the file text\n",
    "    :param code_directive: the name of the directive to search for containing code cells\n",
    "    :param raw_directive: the name of the directive to search for containing raw cells\n",
    "    :param add_source_map: add a `source_map` key to the notebook metadata,\n",
    "        which is a list of the starting source line number for each cell.\n",
    "\n",
    "    :raises MystMetadataParsingError if the metadata block is not valid JSON/YAML\n",
    "\n",
    "    NOTE: we assume here that all of these directives are at the top-level,\n",
    "    i.e. not nested in other directives.\n",
    "    \"\"\"\n",
    "    raise_if_myst_is_not_available()\n",
    "\n",
    "    tokens = get_parser().parse(text + \"\\n\")\n",
    "    lines = text.splitlines()\n",
    "    md_start_line = 0\n",
    "\n",
    "    # get the document metadata\n",
    "    metadata_nb = {}\n",
    "    if tokens and tokens[0].type == \"front_matter\":\n",
    "        metadata = tokens.pop(0)\n",
    "        md_start_line = metadata.map[1]\n",
    "        try:\n",
    "            metadata_nb = yaml.safe_load(metadata.content)\n",
    "        except (yaml.parser.ParserError, yaml.scanner.ScannerError) as error:\n",
    "            raise MystMetadataParsingError(f\"Notebook metadata: {error}\")\n",
    "\n",
    "    # create an empty notebook\n",
    "    nbf_version = nbf.v4\n",
    "    kwargs = {\"metadata\": nbf.from_dict(metadata_nb)}\n",
    "    notebook = nbf_version.new_notebook(**kwargs)\n",
    "    source_map = []  # this is a list of the starting line number for each cell\n",
    "\n",
    "    def _flush_markdown(start_line, token, md_metadata):\n",
    "        \"\"\"When we find a cell we check if there is preceding text.o\"\"\"\n",
    "        endline = token.map[0] if token else len(lines)\n",
    "        md_source = strip_blank_lines(\"\\n\".join(lines[start_line:endline]))\n",
    "        meta = nbf.from_dict(md_metadata)\n",
    "        if md_source:\n",
    "            source_map.append(start_line)\n",
    "            notebook.cells.append(\n",
    "                nbf_version.new_markdown_cell(source=md_source, metadata=meta)\n",
    "            )\n",
    "\n",
    "    # iterate through the tokens to identify notebook cells\n",
    "    nesting_level = 0\n",
    "    md_metadata = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        nesting_level += token.nesting\n",
    "\n",
    "        if nesting_level != 0:\n",
    "            # we ignore fenced block that are nested, e.g. as part of lists, etc\n",
    "            continue\n",
    "\n",
    "        if token.type == \"fence\" and token.info.startswith(code_directive):\n",
    "            _flush_markdown(md_start_line, token, md_metadata)\n",
    "            options, body_lines = read_fenced_cell(token, len(notebook.cells), \"Code\")\n",
    "            meta = nbf.from_dict(options)\n",
    "            source_map.append(token.map[0] + 1)\n",
    "            notebook.cells.append(\n",
    "                nbf_version.new_code_cell(source=\"\\n\".join(body_lines), metadata=meta)\n",
    "            )\n",
    "            md_metadata = {}\n",
    "            md_start_line = token.map[1]\n",
    "\n",
    "        elif token.type == \"fence\" and token.info.startswith(raw_directive):\n",
    "            _flush_markdown(md_start_line, token, md_metadata)\n",
    "            options, body_lines = read_fenced_cell(token, len(notebook.cells), \"Raw\")\n",
    "            meta = nbf.from_dict(options)\n",
    "            source_map.append(token.map[0] + 1)\n",
    "            notebook.cells.append(\n",
    "                nbf_version.new_raw_cell(source=\"\\n\".join(body_lines), metadata=meta)\n",
    "            )\n",
    "            md_metadata = {}\n",
    "            md_start_line = token.map[1]\n",
    "\n",
    "        elif token.type == \"myst_block_break\":\n",
    "            if \"tags\" in md_metadata and \"jp-previous-code-cell-output\" in md_metadata[\"tags\"]:\n",
    "                pass\n",
    "            else:\n",
    "                _flush_markdown(md_start_line, token, md_metadata)\n",
    "            md_metadata = read_cell_metadata(token, len(notebook.cells))\n",
    "            md_start_line = token.map[1]\n",
    "\n",
    "    _flush_markdown(md_start_line, None, md_metadata)\n",
    "\n",
    "    if add_source_map:\n",
    "        notebook.metadata[\"source_map\"] = source_map\n",
    "    return notebook\n",
    "\n",
    "\n",
    "def notebook_to_myst(\n",
    "    nb,\n",
    "    code_directive=CODE_DIRECTIVE,\n",
    "    raw_directive=RAW_DIRECTIVE,\n",
    "    default_lexer=None,\n",
    "):\n",
    "    \"\"\"Parse a notebook to a MyST formatted text document.\n",
    "\n",
    "    :param nb: the notebook to parse\n",
    "    :param code_directive: the name of the directive to use for code cells\n",
    "    :param raw_directive: the name of the directive to use for raw cells\n",
    "    :param default_lexer: a lexer name to use for annotating code cells\n",
    "        (if ``nb.metadata.language_info.pygments_lexer`` is not available)\n",
    "    \"\"\"\n",
    "    raise_if_myst_is_not_available()\n",
    "    string = \"\"\n",
    "\n",
    "    nb_metadata = from_nbnode(nb.metadata)\n",
    "\n",
    "    # we add the pygments lexer as a directive argument, for use by syntax highlighters\n",
    "    pygments_lexer = nb_metadata.get(\"language_info\", {}).get(\"pygments_lexer\", None)\n",
    "    if pygments_lexer is None:\n",
    "        pygments_lexer = default_lexer\n",
    "\n",
    "    if nb_metadata:\n",
    "        string += dump_yaml_blocks(nb_metadata, compact=False)\n",
    "\n",
    "    last_cell_md = False\n",
    "    for i, cell in enumerate(nb.cells):\n",
    "        if cell.cell_type == \"markdown\":\n",
    "            metadata = from_nbnode(cell.metadata)\n",
    "            if metadata or last_cell_md:\n",
    "                if metadata:\n",
    "                    string += f\"\\n+++ {json.dumps(metadata)}\\n\"\n",
    "                else:\n",
    "                    string += \"\\n+++\\n\"\n",
    "            string += \"\\n\" + cell.source\n",
    "            if not cell.source.endswith(\"\\n\"):\n",
    "                string += \"\\n\"\n",
    "            last_cell_md = True\n",
    "\n",
    "        elif cell.cell_type in [\"code\", \"raw\"]:\n",
    "            cell_delimiter = three_backticks_or_more(cell.source.splitlines())\n",
    "            string += \"\\n{}{}\".format(\n",
    "                cell_delimiter,\n",
    "                code_directive if cell.cell_type == \"code\" else raw_directive,\n",
    "            )\n",
    "            if pygments_lexer and cell.cell_type == \"code\":\n",
    "                string += f\" {pygments_lexer}\"\n",
    "            string += \"\\n\"\n",
    "            metadata = from_nbnode(cell.metadata)\n",
    "            if metadata:\n",
    "                string += dump_yaml_blocks(metadata)\n",
    "            elif cell.source.startswith(\"---\") or cell.source.startswith(\":\"):\n",
    "                string += \"\\n\"\n",
    "            string += cell.source\n",
    "            if not cell.source.endswith(\"\\n\"):\n",
    "                string += \"\\n\"\n",
    "            string += cell_delimiter + \"\\n\"\n",
    "            added_md_cell = False\n",
    "            for output in cell.outputs:\n",
    "                if \"data\" in output and \"image/png\" in output[\"data\"]:\n",
    "                    metadata = {\"tags\": [\"jp-previous-code-cell-output\"]}\n",
    "                    string += f\"\\n+++ {json.dumps(metadata)}\\n\"\n",
    "                    string += (\n",
    "                        f'![](data:image/png;base64,{output[\"data\"][\"image/png\"]})\\n'\n",
    "                    )\n",
    "                    added_md_cell = True\n",
    "            last_cell_md = added_md_cell\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"cell {i}, type: {cell.cell_type}\")\n",
    "\n",
    "    return string.rstrip() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytext import read\n",
    "out = notebook_to_myst(read(\"test2r.ipynb\"))\n",
    "#print(out)\n",
    "with open(\"thmyst.md\",\"w\") as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nbformat': 4,\n",
       " 'nbformat_minor': 5,\n",
       " 'metadata': {'jupytext': {'cell_metadata_filter': '-all'},\n",
       "  'kernelspec': {'display_name': 'Python 3 (ipykernel)',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.11.0'}},\n",
       " 'cells': [{'id': '93596e9b',\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '# Another document\\n\\nWith some more text.\\n\\n\\n## Glossary Items\\n\\nWe can define a glossary term as:\\n\\n````text\\n```{glossary}\\nGlossary term one\\n  Glossary term one definition is indented\\n```\\n````\\n\\n```{glossary}\\nGlossary term one\\n  Glossary term one definition is indented\\n```\\n\\nIn OU-XML, the `(term, definition)` pairs are appended to backmatter.\\n\\nWe can then refer to a ``{term}`Glossary term one` TO DO`` that links to the glossary listing.\\n\\n```{glossary}\\nA Glossary term two\\n  Glossary term two definition is indented\\n  No blank line\\n\\n  Blank line\\n\\nGlossary term three\\n  Glossary term three defintion\\n```\\n\\nWhat else?\\n\\nSOme code in a markdon code block:\\n\\n```python\\n# non-executable code\\n# via a markdown code block\\n```',\n",
       "   'metadata': {}},\n",
       "  {'id': 'a4dee62c',\n",
       "   'cell_type': 'code',\n",
       "   'metadata': {'execution': {'iopub.execute_input': '2023-11-21T16:14:03.396246Z',\n",
       "     'iopub.status.busy': '2023-11-21T16:14:03.396033Z',\n",
       "     'iopub.status.idle': '2023-11-21T16:14:04.426371Z',\n",
       "     'shell.execute_reply': '2023-11-21T16:14:04.426110Z'},\n",
       "    'tags': ['hide-input']},\n",
       "   'execution_count': None,\n",
       "   'source': \"# Code in an executable {code-cell} admonition block\\nimport numpy as np\\nimport pandas as pd\\n\\nnp.random.seed(24)\\ndf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\\ndf.plot()\",\n",
       "   'outputs': []},\n",
       "  {'id': 'f075813d',\n",
       "   'cell_type': 'markdown',\n",
       "   'source': 'Images:\\n\\n```{figure} test.png\\n:name: myst_preview\\n:alt: screenshot of MyST sandbox editor\\n:width: 600px\\n\\nScreenshot of MyST sandbox editor with live preview.\\n```\\n\\n\\nAny more?',\n",
       "   'metadata': {}}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myst_to_notebook(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
